{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Datasets/CV0_train_subset_0.csv', index_col=0)\n",
    "y_train= pd.read_csv('Datasets/train_labels.csv', index_col=0).loc[X_train.index][\"damage_grade\"]\n",
    "X_test = pd.read_csv('Datasets/CV0_test_subset_0.csv', index_col=0)\n",
    "y_test= pd.read_csv('Datasets/train_labels.csv', index_col=0).loc[X_test.index][\"damage_grade\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the training of Decision Trees does not take much time to compute, I will optimize the mean score of the 10 subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaceTree  = [Integer(100, 1000, name=\"min_samples_split\"),\n",
    "             Integer(1, 50, name=\"min_samples_leaf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(spaceTree)\n",
    "def objectiveTree(**params):\n",
    "    tree.set_params(**params)\n",
    "    generalization = pd.Series(index=range(10))\n",
    "    for i in generalization.index:\n",
    "        X_train = pd.read_csv('Datasets/CV0_train_subset_{}.csv'.format(i), index_col=0)\n",
    "        y_train= pd.read_csv('Datasets/train_labels.csv', index_col=0).loc[X_train.index][\"damage_grade\"]\n",
    "        X_test = pd.read_csv('Datasets/CV0_test_subset_{}.csv'.format(i), index_col=0)\n",
    "        y_test= pd.read_csv('Datasets/train_labels.csv', index_col=0).loc[X_test.index][\"damage_grade\"]\n",
    "        tree.fit(X_train,y_train)\n",
    "        predict = tree.predict(X_test)\n",
    "        generalization[i] = f1_score(y_test, predict, average='micro')\n",
    "    e = generalization.mean()\n",
    "    print(e, [(a, params[a]) for a in params])\n",
    "    return 1 - e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7168199735239507 [('min_samples_split', 634), ('min_samples_leaf', 42)]\n",
      "0.7160064678527557 [('min_samples_split', 872), ('min_samples_leaf', 43)]\n",
      "0.7166012525133267 [('min_samples_split', 661), ('min_samples_leaf', 20)]\n",
      "0.7174569487379261 [('min_samples_split', 368), ('min_samples_leaf', 4)]\n",
      "0.7178636979660713 [('min_samples_split', 345), ('min_samples_leaf', 24)]\n",
      "0.7161446103057756 [('min_samples_split', 831), ('min_samples_leaf', 25)]\n",
      "0.717230555486106 [('min_samples_split', 454), ('min_samples_leaf', 42)]\n",
      "0.7173073017514285 [('min_samples_split', 404), ('min_samples_leaf', 33)]\n",
      "0.7170079909927364 [('min_samples_split', 431), ('min_samples_leaf', 48)]\n",
      "0.7180440439298754 [('min_samples_split', 226), ('min_samples_leaf', 44)]\n",
      "0.7118084700211732 [('min_samples_split', 100), ('min_samples_leaf', 1)]\n",
      "0.7153004086021333 [('min_samples_split', 985), ('min_samples_leaf', 1)]\n",
      "0.715511456782589 [('min_samples_split', 1000), ('min_samples_leaf', 50)]\n",
      "0.7173725408328997 [('min_samples_split', 100), ('min_samples_leaf', 50)]\n",
      "0.7173687036816008 [('min_samples_split', 100), ('min_samples_leaf', 50)]\n",
      "0.7152543608723885 [('min_samples_split', 979), ('min_samples_leaf', 1)]\n",
      "0.7173418424445652 [('min_samples_split', 100), ('min_samples_leaf', 50)]\n",
      "0.7124262705218759 [('min_samples_split', 106), ('min_samples_leaf', 1)]\n",
      "0.7154807583942544 [('min_samples_split', 997), ('min_samples_leaf', 50)]\n",
      "0.7152505235738466 [('min_samples_split', 981), ('min_samples_leaf', 1)]\n",
      "0.7173456798903499 [('min_samples_split', 100), ('min_samples_leaf', 50)]\n",
      "0.7154807583942544 [('min_samples_split', 996), ('min_samples_leaf', 50)]\n",
      "0.7173571916387325 [('min_samples_split', 100), ('min_samples_leaf', 50)]\n",
      "0.7173763781314415 [('min_samples_split', 100), ('min_samples_leaf', 50)]\n",
      "0.7173341678474816 [('min_samples_split', 100), ('min_samples_leaf', 50)]\n",
      "0.7123878970947287 [('min_samples_split', 106), ('min_samples_leaf', 1)]\n",
      "0.7154462227073779 [('min_samples_split', 986), ('min_samples_leaf', 50)]\n",
      "0.715511456782589 [('min_samples_split', 1000), ('min_samples_leaf', 50)]\n",
      "0.7122727787274457 [('min_samples_split', 104), ('min_samples_leaf', 1)]\n",
      "0.715377154425727 [('min_samples_split', 997), ('min_samples_leaf', 1)]\n",
      "0.7154769210957126 [('min_samples_split', 996), ('min_samples_leaf', 50)]\n",
      "0.7173303306961827 [('min_samples_split', 100), ('min_samples_leaf', 50)]\n",
      "0.71734567989035 [('min_samples_split', 100), ('min_samples_leaf', 50)]\n",
      "0.715377154425727 [('min_samples_split', 999), ('min_samples_leaf', 1)]\n",
      "0.7133395444827285 [('min_samples_split', 103), ('min_samples_leaf', 2)]\n",
      "0.7154769210957126 [('min_samples_split', 997), ('min_samples_leaf', 50)]\n",
      "0.7153502933359343 [('min_samples_split', 993), ('min_samples_leaf', 1)]\n",
      "0.7155076194840471 [('min_samples_split', 999), ('min_samples_leaf', 50)]\n",
      "0.7124416205995008 [('min_samples_split', 108), ('min_samples_leaf', 1)]\n",
      "0.7155076194840471 [('min_samples_split', 998), ('min_samples_leaf', 50)]\n",
      "0.7173571916387325 [('min_samples_split', 100), ('min_samples_leaf', 50)]\n",
      "0.7173380052932663 [('min_samples_split', 100), ('min_samples_leaf', 50)]\n",
      "0.7153387815875517 [('min_samples_split', 994), ('min_samples_leaf', 1)]\n",
      "0.7133280311146735 [('min_samples_split', 120), ('min_samples_leaf', 1)]\n",
      "0.715511456782589 [('min_samples_split', 999), ('min_samples_leaf', 50)]\n",
      "0.7153694798286433 [('min_samples_split', 999), ('min_samples_leaf', 1)]\n",
      "0.7173533544874335 [('min_samples_split', 100), ('min_samples_leaf', 50)]\n",
      "0.7155076194840471 [('min_samples_split', 999), ('min_samples_leaf', 50)]\n",
      "0.7122344060365133 [('min_samples_split', 104), ('min_samples_leaf', 1)]\n",
      "0.7154845956927962 [('min_samples_split', 990), ('min_samples_leaf', 50)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7180440439298754, 226, 44)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(criterion='gini', splitter='best', min_samples_split=None, min_samples_leaf=None)\n",
    "res_tree = gp_minimize(objectiveTree, spaceTree, n_calls=50, random_state=0)\n",
    "1-res_tree.fun, res_tree.x[0], res_tree.x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be clearer, I have previously narrowed the space search. Here is displayed the final step. Random forest training takes a lot of time to compute, so I will optimize only subset0 and then see if it generalizes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaceForest  = [Integer(2, 30, name=\"min_samples_split\"),\n",
    "                Integer(1, 5, name=\"min_samples_leaf\"),\n",
    "                Integer(700, 2000, name=\"n_estimators\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(spaceForest)\n",
    "def objectiveForest(**params):\n",
    "    forest.set_params(**params)\n",
    "    forest.fit(X_train,y_train)\n",
    "    predict = forest.predict(X_test)\n",
    "    e = f1_score(y_test, predict, average='micro')\n",
    "    print(e, [(a, params[a]) for a in params])\n",
    "    return 1 - e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.735111281657713 [('min_samples_split', 19), ('min_samples_leaf', 4), ('n_estimators', 1815)]\n",
      "0.7344205679201842 [('min_samples_split', 26), ('min_samples_leaf', 3), ('n_estimators', 1200)]\n",
      "0.7338066001534919 [('min_samples_split', 10), ('min_samples_leaf', 1), ('n_estimators', 1054)]\n",
      "0.7348426707597853 [('min_samples_split', 15), ('min_samples_leaf', 4), ('n_estimators', 1324)]\n",
      "0.7351496546431312 [('min_samples_split', 13), ('min_samples_leaf', 4), ('n_estimators', 1139)]\n",
      "0.736415963161934 [('min_samples_split', 20), ('min_samples_leaf', 2), ('n_estimators', 1944)]\n",
      "0.7361089792785878 [('min_samples_split', 6), ('min_samples_leaf', 4), ('n_estimators', 1316)]\n",
      "0.7352264006139678 [('min_samples_split', 24), ('min_samples_leaf', 3), ('n_estimators', 1583)]\n",
      "0.7354950115118957 [('min_samples_split', 22), ('min_samples_leaf', 3), ('n_estimators', 1399)]\n",
      "0.7356485034535687 [('min_samples_split', 23), ('min_samples_leaf', 1), ('n_estimators', 1316)]\n",
      "0.7353415195702225 [('min_samples_split', 2), ('min_samples_leaf', 5), ('n_estimators', 2000)]\n",
      "0.7339984650805833 [('min_samples_split', 30), ('min_samples_leaf', 5), ('n_estimators', 700)]\n",
      "0.7163085188027628 [('min_samples_split', 2), ('min_samples_leaf', 1), ('n_estimators', 756)]\n",
      "0.7326554105909441 [('min_samples_split', 30), ('min_samples_leaf', 5), ('n_estimators', 2000)]\n",
      "0.7159631619339984 [('min_samples_split', 2), ('min_samples_leaf', 1), ('n_estimators', 1997)]\n",
      "0.7325786646201075 [('min_samples_split', 30), ('min_samples_leaf', 5), ('n_estimators', 808)]\n",
      "0.7354950115118957 [('min_samples_split', 30), ('min_samples_leaf', 1), ('n_estimators', 993)]\n",
      "0.7354566385264774 [('min_samples_split', 12), ('min_samples_leaf', 5), ('n_estimators', 710)]\n",
      "0.7358403683806601 [('min_samples_split', 30), ('min_samples_leaf', 1), ('n_estimators', 2000)]\n",
      "0.733384497313891 [('min_samples_split', 30), ('min_samples_leaf', 5), ('n_estimators', 1881)]\n",
      "0.7327321565617806 [('min_samples_split', 30), ('min_samples_leaf', 5), ('n_estimators', 708)]\n",
      "0.733384497313891 [('min_samples_split', 30), ('min_samples_leaf', 5), ('n_estimators', 1952)]\n",
      "0.7355333844973139 [('min_samples_split', 30), ('min_samples_leaf', 1), ('n_estimators', 712)]\n",
      "0.7348810437452034 [('min_samples_split', 12), ('min_samples_leaf', 5), ('n_estimators', 2000)]\n",
      "0.7330775134305448 [('min_samples_split', 30), ('min_samples_leaf', 5), ('n_estimators', 749)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.736415963161934, 20, 2, 1944)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv('Datasets/CV0_train_subset_0.csv', index_col=0)\n",
    "y_train= pd.read_csv('Datasets/train_labels.csv', index_col=0).loc[X_train.index][\"damage_grade\"]\n",
    "X_test = pd.read_csv('Datasets/CV0_test_subset_0.csv', index_col=0)\n",
    "y_test= pd.read_csv('Datasets/train_labels.csv', index_col=0).loc[X_test.index][\"damage_grade\"]\n",
    "forest = RandomForestClassifier(min_samples_split=225, min_samples_leaf=45, n_jobs=-1)\n",
    "res_forest = gp_minimize(objectiveForest, spaceForest, n_calls=25, random_state=0)\n",
    "1-res_forest.fun, res_forest.x[0], res_forest.x[1], res_forest.x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I must pay attention here : if the best model would have been the most prone to overfitting (2000 estimators, 1 min samples leaf and 1 samples split) it would have been a clue that I were in the wrong direction. Fortunately, the optimization find a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.732924\n",
       "1    0.734152\n",
       "2    0.730200\n",
       "3    0.728665\n",
       "4    0.736685\n",
       "5    0.727974\n",
       "6    0.735303\n",
       "7    0.732579\n",
       "8    0.732348\n",
       "9    0.732359\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generalization = pd.Series(index=range(10))\n",
    "for i in generalization.index:\n",
    "    forest = RandomForestClassifier(min_samples_split=res_forest.x[0], min_samples_leaf=res_forest.x[1],\n",
    "                                    n_estimators=res_forest.x[0], n_jobs=-1)\n",
    "    X_train = pd.read_csv('Datasets/CV0_train_subset_{}.csv'.format(i), index_col=0)\n",
    "    y_train= pd.read_csv('Datasets/train_labels.csv', index_col=0).loc[X_train.index][\"damage_grade\"]\n",
    "    X_test = pd.read_csv('Datasets/CV0_test_subset_{}.csv'.format(i), index_col=0)\n",
    "    y_test= pd.read_csv('Datasets/train_labels.csv', index_col=0).loc[X_test.index][\"damage_grade\"]\n",
    "    forest.fit(X_train,y_train)\n",
    "    predict = forest.predict(X_test)\n",
    "    generalization[i] = f1_score(y_test, predict, average='micro')\n",
    "generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalization is good, and this is my best model so far !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest neighbors classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Datasets/CV0_train_subset_0.csv', index_col=0)\n",
    "y_train= pd.read_csv('Datasets/train_labels.csv', index_col=0).loc[X_train.index][\"damage_grade\"]\n",
    "X_test = pd.read_csv('Datasets/CV0_test_subset_0.csv', index_col=0)\n",
    "y_test= pd.read_csv('Datasets/train_labels.csv', index_col=0).loc[X_test.index][\"damage_grade\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be clearer, I have previously narrowed the space search. Here is displayed the final step. Nearest neighbors classsifiers training takes a lot of time to compute, so I will optimize only subset0 and then see if it generalizes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaceKNN  = [Integer(20, 30, name=\"n_neighbors\"),\n",
    "             Integer(25, 55, name=\"leaf_size\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(spaceKNN)\n",
    "def objectiveKNN(**params):\n",
    "    knn.set_params(**params)\n",
    "    knn.fit(X_train,y_train)\n",
    "    predict = knn.predict(X_test)\n",
    "    e = f1_score(y_test, predict, average='micro')\n",
    "    print(e, [(a, params[a]) for a in params])\n",
    "    return 1 - e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=2, weights='uniform', algorithm='auto', leaf_size=10, p=2,\n",
    "                            metric='minkowski', metric_params=None, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.709669992325403 [('n_neighbors', 26), ('leaf_size', 50)]\n",
      "0.7080583269378358 [('n_neighbors', 29), ('leaf_size', 50)]\n",
      "0.709669992325403 [('n_neighbors', 26), ('leaf_size', 37)]\n",
      "0.7081734458940907 [('n_neighbors', 23), ('leaf_size', 27)]\n",
      "0.7082118188795087 [('n_neighbors', 23), ('leaf_size', 39)]\n",
      "0.7085571757482733 [('n_neighbors', 28), ('leaf_size', 39)]\n",
      "0.7089025326170376 [('n_neighbors', 24), ('leaf_size', 50)]\n",
      "0.7082118188795087 [('n_neighbors', 23), ('leaf_size', 44)]\n",
      "0.7089025326170376 [('n_neighbors', 24), ('leaf_size', 54)]\n",
      "0.7066385264773599 [('n_neighbors', 21), ('leaf_size', 51)]\n",
      "0.7080199539524175 [('n_neighbors', 30), ('leaf_size', 25)]\n",
      "0.7082885648503453 [('n_neighbors', 20), ('leaf_size', 25)]\n",
      "0.7080199539524175 [('n_neighbors', 30), ('leaf_size', 25)]\n",
      "0.7081734458940907 [('n_neighbors', 30), ('leaf_size', 55)]\n",
      "0.7082885648503453 [('n_neighbors', 20), ('leaf_size', 25)]\n",
      "0.7080199539524175 [('n_neighbors', 20), ('leaf_size', 55)]\n",
      "0.7080199539524175 [('n_neighbors', 30), ('leaf_size', 25)]\n",
      "0.7081734458940907 [('n_neighbors', 30), ('leaf_size', 55)]\n",
      "0.7082885648503453 [('n_neighbors', 20), ('leaf_size', 25)]\n",
      "0.7080199539524175 [('n_neighbors', 20), ('leaf_size', 55)]\n",
      "0.7080199539524175 [('n_neighbors', 30), ('leaf_size', 25)]\n",
      "0.7081734458940907 [('n_neighbors', 30), ('leaf_size', 55)]\n",
      "0.7082885648503453 [('n_neighbors', 20), ('leaf_size', 25)]\n",
      "0.7080199539524175 [('n_neighbors', 20), ('leaf_size', 55)]\n",
      "0.7080199539524175 [('n_neighbors', 30), ('leaf_size', 25)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.709669992325403, 26, 50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_KNN = gp_minimize(objectiveKNN, spaceKNN, n_calls=25, random_state=0)\n",
    "1-res_KNN.fun, res_KNN.x[0], res_KNN.x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.709670\n",
       "1    0.706523\n",
       "2    0.707598\n",
       "3    0.704873\n",
       "4    0.710860\n",
       "5    0.707483\n",
       "6    0.709823\n",
       "7    0.709785\n",
       "8    0.709593\n",
       "9    0.707149\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generalization = pd.Series(index=range(10))\n",
    "for i in generalization.index:\n",
    "    knn = KNeighborsClassifier(n_neighbors=res_KNN.x[0], weights='uniform', algorithm='auto', leaf_size=res_KNN.x[1], p=2,\n",
    "                            metric='minkowski', metric_params=None, n_jobs=-1)\n",
    "    X_train = pd.read_csv('Datasets/CV0_train_subset_{}.csv'.format(i), index_col=0)\n",
    "    y_train= pd.read_csv('Datasets/train_labels.csv', index_col=0).loc[X_train.index][\"damage_grade\"]\n",
    "    X_test = pd.read_csv('Datasets/CV0_test_subset_{}.csv'.format(i), index_col=0)\n",
    "    y_test= pd.read_csv('Datasets/train_labels.csv', index_col=0).loc[X_test.index][\"damage_grade\"]\n",
    "    knn.fit(X_train,y_train)\n",
    "    predict = knn.predict(X_test)\n",
    "    generalization[i] = f1_score(y_test, predict, average='micro')\n",
    "generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalization is very good. However, I expected better results from this model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
