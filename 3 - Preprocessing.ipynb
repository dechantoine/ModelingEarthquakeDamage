{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Datasets/train_values.csv\").set_index('building_id')\n",
    "damages = pd.read_csv(\"Datasets/train_labels.csv\").set_index('building_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will use the conclusion from exploration to process the dataset. I will create 10 crossvalidation subsets. Each subset will be process as a train set and as a test set (with slight changes between the 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function substitutes geo level 1&2&3 by the expected damage value of geo level 2.\n",
    "def geo_level_process(train, test, damages):\n",
    "    dmgs = df.merge(damages, right_index=True, left_index=True).loc[:,['geo_level_2_id','damage_grade']]\n",
    "    expected_value = pd.DataFrame(index=dmgs['geo_level_2_id'].unique(), columns=[\"expected_value\", \"ratio\"])\n",
    "    for i in expected_value.index:\n",
    "        geo = dmgs[dmgs['geo_level_2_id']==i]\n",
    "        expected_value.loc[i,\"expected_value\"] = geo['damage_grade'].mean()\n",
    "        expected_value.loc[i, \"ratio\"] =  len(geo[geo['damage_grade']==2])/len(geo)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(expected_value[\"expected_value\"].values.reshape(-1, 1))\n",
    "    train['geo_level_mean'] = scaler.transform(train['geo_level_2_id'].apply(lambda x: expected_value.loc[x,\"expected_value\"]).values.reshape(-1, 1))\n",
    "    test['geo_level_mean'] = scaler.transform(test['geo_level_2_id'].apply(lambda x: expected_value.loc[x,\"expected_value\"]).values.reshape(-1, 1))\n",
    "    scaler.fit(expected_value[\"ratio\"].values.reshape(-1, 1))\n",
    "    train['geo_level_ratio'] = scaler.transform(train['geo_level_2_id'].apply(lambda x: expected_value.loc[x,\"ratio\"]).values.reshape(-1, 1))\n",
    "    test['geo_level_ratio'] = scaler.transform(test['geo_level_2_id'].apply(lambda x: expected_value.loc[x,\"ratio\"]).values.reshape(-1, 1))\n",
    "    train = train.drop(columns=['geo_level_1_id','geo_level_2_id','geo_level_3_id'])\n",
    "    test = test.drop(columns=['geo_level_1_id','geo_level_2_id','geo_level_3_id'])\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function removes buildings with more than 7 floors and applies a 0 to 1 scaler\n",
    "def count_floors_process(train, test, damages):\n",
    "    train = train.drop(index=train[train[\"count_floors_pre_eq\"] > 7].index)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(train[\"count_floors_pre_eq\"].values.reshape(-1, 1))\n",
    "    train[\"count_floors_pre_eq\"] = scaler.transform(train[\"count_floors_pre_eq\"].values.reshape(-1, 1))  \n",
    "    test[\"count_floors_pre_eq\"] = scaler.transform(test[\"count_floors_pre_eq\"].values.reshape(-1, 1))  \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function removes buildings aged more than 100 years and applies a 0 to 1 scaler\n",
    "def age_process(train, test, damages):\n",
    "    train = train.drop(index=train[train[\"age\"] > 100].index)\n",
    "    test[\"age\"] = test[\"age\"].apply(lambda x: 100 if x>100 else x)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(train[\"age\"].values.reshape(-1, 1))\n",
    "    train[\"age\"] = scaler.transform(train[\"age\"].values.reshape(-1, 1))\n",
    "    test[\"age\"] = scaler.transform(test[\"age\"].values.reshape(-1, 1))   \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function merges together buildings with area percentage >= 17 and applies a 0 to 1 scaler\n",
    "def area_process(train, test, damages):\n",
    "    train[\"area_percentage\"] = train[\"area_percentage\"].apply(lambda x: 17 if x>16 else x)\n",
    "    test[\"area_percentage\"] = test[\"area_percentage\"].apply(lambda x: 17 if x>16 else x)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(train[\"area_percentage\"].values.reshape(-1, 1))\n",
    "    train[\"area_percentage\"] = scaler.transform(train[\"area_percentage\"].values.reshape(-1, 1))\n",
    "    test[\"area_percentage\"] = scaler.transform(test[\"area_percentage\"].values.reshape(-1, 1))   \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function applies a 0 to 1 scaler to height\n",
    "def height_process(train, test, damages):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(train[\"height_percentage\"].values.reshape(-1, 1))\n",
    "    train[\"height_percentage\"] = scaler.transform(train[\"height_percentage\"].values.reshape(-1, 1))\n",
    "    test[\"height_percentage\"] = scaler.transform(test[\"height_percentage\"].values.reshape(-1, 1))\n",
    "    return train ,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function applies one hot encoder (categories become binary features) to land_surface_condition\n",
    "def land_surface_process(train, test, damages):\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe.fit(train[\"land_surface_condition\"].values.reshape(-1, 1))\n",
    "    train_encoded = pd.DataFrame(ohe.transform(train[\"land_surface_condition\"].values.reshape(-1, 1)).toarray(), \n",
    "                                 index=train.index, columns=[\"land_surface_condition_\" + i for i in ohe.categories_[0]])\n",
    "    test_encoded = pd.DataFrame(ohe.transform(test[\"land_surface_condition\"].values.reshape(-1, 1)).toarray(), \n",
    "                                index=test.index, columns=[\"land_surface_condition_\" + i for i in ohe.categories_[0]])\n",
    "    train = train.merge(train_encoded, right_index=True, left_index=True).drop(columns=\"land_surface_condition\")\n",
    "    test = test.merge(test_encoded, right_index=True, left_index=True).drop(columns=\"land_surface_condition\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function applies one hot encoder (categories become binary features) to fundation_type\n",
    "def fundation_process(train, test, damages):\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe.fit(train[\"foundation_type\"].values.reshape(-1, 1))\n",
    "    train_encoded = pd.DataFrame(ohe.transform(train[\"foundation_type\"].values.reshape(-1, 1)).toarray(), \n",
    "                                 index=train.index, columns=[\"foundation_type_\" + i for i in ohe.categories_[0]])\n",
    "    test_encoded = pd.DataFrame(ohe.transform(test[\"foundation_type\"].values.reshape(-1, 1)).toarray(),\n",
    "                                index=test.index, columns=[\"foundation_type_\" + i for i in ohe.categories_[0]])\n",
    "    train = train.merge(train_encoded, right_index=True, left_index=True).drop(columns=\"foundation_type\")\n",
    "    test = test.merge(test_encoded, right_index=True, left_index=True).drop(columns=\"foundation_type\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function applies one hot encoder (categories become binary features) to roof_type\n",
    "def roof_process(train, test, damages):\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe.fit(train[\"roof_type\"].values.reshape(-1, 1))\n",
    "    train_encoded = pd.DataFrame(ohe.transform(train[\"roof_type\"].values.reshape(-1, 1)).toarray(), \n",
    "                                 index=train.index, columns=[\"roof_type_\" + i for i in ohe.categories_[0]])\n",
    "    test_encoded = pd.DataFrame(ohe.transform(test[\"roof_type\"].values.reshape(-1, 1)).toarray(),\n",
    "                                index=test.index, columns=[\"roof_type_\" + i for i in ohe.categories_[0]])\n",
    "    train = train.merge(train_encoded, right_index=True, left_index=True).drop(columns=\"roof_type\")\n",
    "    test = test.merge(test_encoded, right_index=True, left_index=True).drop(columns=\"roof_type\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function first merges together categories m and z, then applies one hot encoder\n",
    "# (categories become binary features) to ground_floor_type\n",
    "def ground_floor_process(train, test, damages):\n",
    "    ohe = OneHotEncoder()\n",
    "    train[\"ground_floor_type\"] = train[\"ground_floor_type\"].apply(lambda x: \"z\" if x==\"m\" else x)\n",
    "    test[\"ground_floor_type\"] = test[\"ground_floor_type\"].apply(lambda x: \"z\" if x==\"m\" else x)\n",
    "    ohe.fit(train[\"ground_floor_type\"].values.reshape(-1, 1))\n",
    "    train_encoded = pd.DataFrame(ohe.transform(train[\"ground_floor_type\"].values.reshape(-1, 1)).toarray(), \n",
    "                                 index=train.index, columns=[\"ground_floor_type_\" + i for i in ohe.categories_[0]])\n",
    "    test_encoded = pd.DataFrame(ohe.transform(test[\"ground_floor_type\"].values.reshape(-1, 1)).toarray(), \n",
    "                                 index=test.index, columns=[\"ground_floor_type_\" + i for i in ohe.categories_[0]])\n",
    "    train = train.merge(train_encoded, right_index=True, left_index=True).drop(columns=\"ground_floor_type\")\n",
    "    test = test.merge(test_encoded, right_index=True, left_index=True).drop(columns=\"ground_floor_type\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function applies one hot encoder (categories become binary features) to other_floor_type\n",
    "def other_floor_process(train, test, damages):\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe.fit(train[\"other_floor_type\"].values.reshape(-1, 1))\n",
    "    train_encoded = pd.DataFrame(ohe.transform(train[\"other_floor_type\"].values.reshape(-1, 1)).toarray(), \n",
    "                                 index=train.index, columns=[\"other_floor_type_\" + i for i in ohe.categories_[0]])\n",
    "    test_encoded = pd.DataFrame(ohe.transform(test[\"other_floor_type\"].values.reshape(-1, 1)).toarray(), \n",
    "                                 index=test.index, columns=[\"other_floor_type_\" + i for i in ohe.categories_[0]])\n",
    "    train = train.merge(train_encoded, right_index=True, left_index=True).drop(columns=\"other_floor_type\")\n",
    "    test = test.merge(test_encoded, right_index=True, left_index=True).drop(columns=\"other_floor_type\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function applies one hot encoder (categories become binary features) to position\n",
    "def position_process(train, test, damages):\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe.fit(train[\"position\"].values.reshape(-1, 1))\n",
    "    train_encoded = pd.DataFrame(ohe.transform(train[\"position\"].values.reshape(-1, 1)).toarray(), \n",
    "                                 index=train.index, columns=[\"position_\" + i for i in ohe.categories_[0]])\n",
    "    test_encoded = pd.DataFrame(ohe.transform(test[\"position\"].values.reshape(-1, 1)).toarray(), \n",
    "                                 index=test.index, columns=[\"position_\" + i for i in ohe.categories_[0]])\n",
    "    train = train.merge(train_encoded, right_index=True, left_index=True).drop(columns=\"position\")\n",
    "    test = test.merge(test_encoded, right_index=True, left_index=True).drop(columns=\"position\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function this function replace each plan_configuration category by 2 new features : its mean and \n",
    "# its ratio len(damages==2)/len(category). It then applies a 0 to 1 scaler to those new features.\n",
    "def plan_configuration_process(train, test, damages):\n",
    "    dmgs = df.merge(damages, right_index=True, left_index=True).loc[:,['plan_configuration','damage_grade']]\n",
    "    expected_value = pd.DataFrame(index=dmgs['plan_configuration'].unique(), columns=[\"expected_value\", \"ratio\"])\n",
    "    for i in expected_value.index:\n",
    "        category = dmgs[dmgs['plan_configuration']==i]\n",
    "        expected_value.loc[i,\"expected_value\"] = category['damage_grade'].mean()\n",
    "        expected_value.loc[i, \"ratio\"] =  len(category[category['damage_grade']==2])/len(category)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(expected_value[\"expected_value\"].values.reshape(-1, 1))\n",
    "    train['plan_configuration_mean'] = scaler.transform(train['plan_configuration'].apply(lambda x: expected_value.loc[x,\"expected_value\"]).values.reshape(-1, 1))\n",
    "    test['plan_configuration_mean'] = scaler.transform(test['plan_configuration'].apply(lambda x: expected_value.loc[x,\"expected_value\"]).values.reshape(-1, 1))\n",
    "    scaler.fit(expected_value[\"ratio\"].values.reshape(-1, 1))\n",
    "    train['plan_configuration_ratio'] = scaler.transform(train['plan_configuration'].apply(lambda x: expected_value.loc[x,\"ratio\"]).values.reshape(-1, 1))\n",
    "    test['plan_configuration_ratio'] = scaler.transform(test['plan_configuration'].apply(lambda x: expected_value.loc[x,\"ratio\"]).values.reshape(-1, 1))\n",
    "    train = train.drop(columns=\"plan_configuration\")\n",
    "    test = test.drop(columns=\"plan_configuration\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I will merge together several secondary use.\n",
    "def secondary_use_process(train, test, damages):\n",
    "    train.head()\n",
    "    train[\"secondary_use_1\"]= (train[\"has_secondary_use_institution\"] +\n",
    "                               train[\"has_secondary_use_rental\"]).apply(lambda x: 1 if x>0 else 0)\n",
    "    test[\"secondary_use_1\"]= (test[\"has_secondary_use_institution\"] +\n",
    "                              test[\"has_secondary_use_rental\"]).apply(lambda x: 1 if x>0 else 0)\n",
    "    train[\"secondary_use_2\"]= (train[\"has_secondary_use_health_post\"] + train[\"has_secondary_use_school\"] +\n",
    "                               train[\"has_secondary_use_gov_office\"]).apply(lambda x: 1 if x>0 else 0)\n",
    "    test[\"secondary_use_2\"]= (test[\"has_secondary_use_health_post\"] + test[\"has_secondary_use_school\"] +\n",
    "                              test[\"has_secondary_use_gov_office\"]).apply(lambda x: 1 if x>0 else 0)\n",
    "    train[\"secondary_use_3\"]= (train[\"has_secondary_use_hotel\"] + train[\"has_secondary_use_industry\"] +\n",
    "                               train[\"has_secondary_use_other\"]).apply(lambda x: 1 if x>0 else 0)\n",
    "    test[\"secondary_use_3\"]= (test[\"has_secondary_use_hotel\"] + test[\"has_secondary_use_industry\"] +\n",
    "                              test[\"has_secondary_use_other\"]).apply(lambda x: 1 if x>0 else 0)\n",
    "    train = train.drop(columns=[\"has_secondary_use\", \"has_secondary_use_institution\", \"has_secondary_use_rental\",\n",
    "                               \"has_secondary_use_health_post\", \"has_secondary_use_school\", \"has_secondary_use_gov_office\",\n",
    "                               \"has_secondary_use_hotel\", \"has_secondary_use_industry\", \"has_secondary_use_other\"]) \n",
    "    test = test.drop(columns=[\"has_secondary_use\", \"has_secondary_use_institution\", \"has_secondary_use_rental\",\n",
    "                              \"has_secondary_use_health_post\", \"has_secondary_use_school\", \"has_secondary_use_gov_office\",\n",
    "                              \"has_secondary_use_hotel\", \"has_secondary_use_industry\", \"has_secondary_use_other\"])\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-fold cross validation subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    testCV = df.iloc[int(len(df)*0.1*i):int(len(df)*0.1*(i+1)),:]\n",
    "    trainCV = df.drop(index=testCV.index)\n",
    "    trainCV, testCV = geo_level_process(trainCV, testCV, damages)\n",
    "    trainCV, testCV = count_floors_process(trainCV, testCV, damages)\n",
    "    trainCV, testCV = age_process(trainCV, testCV, damages)\n",
    "    trainCV, testCV = area_process(trainCV, testCV, damages)\n",
    "    trainCV, testCV = height_process(trainCV, testCV, damages)\n",
    "    trainCV, testCV = trainCV.drop(columns=\"count_families\"), testCV.drop(columns=\"count_families\")\n",
    "    trainCV, testCV = land_surface_process(trainCV, testCV, damages)\n",
    "    trainCV, testCV = fundation_process(trainCV, testCV, damages)\n",
    "    trainCV, testCV = roof_process(trainCV, testCV, damages)\n",
    "    trainCV, testCV = ground_floor_process(trainCV, testCV, damages)\n",
    "    trainCV, testCV = other_floor_process(trainCV, testCV, damages)\n",
    "    trainCV, testCV = position_process(trainCV, testCV, damages)\n",
    "    trainCV, testCV = plan_configuration_process(trainCV, testCV, damages)\n",
    "    trainCV, testCV = trainCV.drop(columns=\"legal_ownership_status\"), testCV.drop(columns=\"legal_ownership_status\")\n",
    "    trainCV, testCV = secondary_use_process(trainCV, testCV, damages)\n",
    "    trainCV.to_csv('Datasets/CV0_train_subset_{}.csv'.format(i))\n",
    "    testCV.to_csv('Datasets/CV0_test_subset_{}.csv'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Damages scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression purposes, I will use scaled targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "damages = (damages - 1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "damages.to_csv('Datasets/train_labels_scaled.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
